{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark import SparkContext\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "import redis \n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "def get_id(line):\n",
    "    parenthesis = \"\\\"\" # string literal for \"\n",
    "    paper_id_tag = \"\\\"id\\\"\" # find the first occurence of \"id\"\n",
    "    id_label_start = line.find(paper_id_tag) # this is the index that the id label starts\n",
    "    id_tag_start = id_label_start + 6 # this is the index that the id tag starts. Always be 6.\n",
    "    id_tag_end = line.find(parenthesis, id_tag_start)  #this is the index that the id tag ends\n",
    "    id_tag = line[id_tag_start:id_tag_end] # id tag string \n",
    "    return id_tag\n",
    "\n",
    "def get_abstract(line):\n",
    "    parenthesis = \"\\\"\" # string literal for \"\n",
    "    paper_abstract_tag = \"\\\"paperAbstract\\\"\"\n",
    "    abstract_label_start = line.find(paper_abstract_tag) # index that the abstrat label starts \n",
    "    abstract_tag_start = abstract_label_start + 17 # the start of the abstract tag \n",
    "    abstract_tag_end = line.find(parenthesis, abstract_tag_start) # the end of the abstract tag\n",
    "    abstract_tag = line[abstract_tag_start:abstract_tag_end] # abstract tag string \n",
    "    return abstract_tag\n",
    "\n",
    "def adding_ids(df):\n",
    "    '''\n",
    "    This function takes the raw data dataframe and adds on an id column for the data\n",
    "    Ex: \n",
    "    value        id \n",
    "    laeinaelk    23402939423\n",
    "    lakeflake    02398402384\n",
    "    ieifniena    23402938402\n",
    "    '''\n",
    "    add_ids = df.withColumn(\"id\", get_id_udf(raw_data.value))\n",
    "    return add_ids\n",
    "\n",
    "def adding_abstracts(df):\n",
    "    '''\n",
    "    This function takes the raw + id dataframe and adds on abstracts column for the data\n",
    "    Ex\n",
    "    value        id             abstracts\n",
    "    laeinaelk    23402939423    Mastering the game of ...\n",
    "    lakeflake    02398402384    When people go outside...\n",
    "    ieifniena    23402938402    Data engineers love to...\n",
    "    '''\n",
    "    add_ids_abstracts = df.withColumn(\"abstracts\", get_abstract_udf(raw_data.value))\n",
    "    return add_ids_abstracts\n",
    "\n",
    "# create a user defined function for get_id and get_abstract, which is compatable with a spark dataframe \n",
    "get_id_udf = udf(lambda line: get_id(line), StringType())\n",
    "get_abstract_udf = udf(lambda line: get_abstract(line), StringType())\n",
    "\n",
    "# read in the raw data file \n",
    "filenames = \"s3a://open-research-corpus/sample-S2-records.gz\" # path to the example file from S3 file \n",
    "raw_data = spark.read.text(filenames)\n",
    "raw_and_ids = adding_ids(raw_data)\n",
    "raw_ids_abstracts = adding_abstracts(raw_and_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for raw data + ids + abstracts\n",
      "-------------------------------------\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- abstracts: string (nullable = true)\n",
      "\n",
      "First 5 entries for add_ids_abstracts data\n",
      "----------------------------\n",
      "+--------------------+--------------------+--------------------+\n",
      "|               value|                  id|           abstracts|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|{\"entities\":[\"Epi...|4cbba8127c8747a3b...|Primary debulking...|\n",
      "|{\"entities\":[\"Lip...|4c61478345166be0d...|                    |\n",
      "|{\"entities\":[\"Spa...|34ca6d85db744543d...|                    |\n",
      "|{\"entities\":[\"ACT...|3316b8b97c1e17ac9...|Experiments were ...|\n",
      "|{\"entities\":[],\"j...|58ff17c7d8ca00673...|                    |\n",
      "|{\"entities\":[\"Alg...|f487c60cc4b463758...|                    |\n",
      "|{\"entities\":[\"Acc...|69662bd2a2f5ff9cf...|The potential to ...|\n",
      "|{\"entities\":[\"Wor...|3f1b5aa320422a4df...|Emotions are an o...|\n",
      "|{\"entities\":[\"Emb...|453ae606c4c5f2dd2...|We demonstrate tu...|\n",
      "|{\"entities\":[\"Tec...|c739d07173f366ba9...|                    |\n",
      "|{\"entities\":[\"All...|cb61fc1ebdeb58354...|A scald burn inju...|\n",
      "|{\"entities\":[\"Ace...|50ef31b58a30dfefb...|Megakaryocytes ar...|\n",
      "|{\"entities\":[\"ALD...|10b5e8d1ab6f8002c...|Time-dependent de...|\n",
      "|{\"entities\":[\"Bro...|ccc67f73db54afc3f...|In this paper, we...|\n",
      "|{\"entities\":[],\"j...|7e1dc0e805fbb1311...|OBJECTIVE\\nTo pre...|\n",
      "|{\"entities\":[\"Wax...|7331b32342d54e97c...|                    |\n",
      "|{\"entities\":[\"Ale...|177e662ac662c21b0...|BACKGROUND\\nUnila...|\n",
      "|{\"entities\":[\"Cri...|ac1ccf2f8d373a496...|Until recently th...|\n",
      "|{\"entities\":[\"Aci...|06f3d20b2c9191b4f...|The analysis of c...|\n",
      "|{\"entities\":[],\"j...|b048c1886c86e65fd...|The toxicokinetic...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Schema for raw data + ids + abstracts\")\n",
    "print(\"-------------------------------------\")\n",
    "raw_ids_abstracts.createOrReplaceTempView(\"raw_ids_and_abstracts\")\n",
    "raw_ids_abstracts.printSchema()\n",
    "results = spark.sql(\"SELECT * FROM raw_ids_and_abstracts\")\n",
    "print(\"First 5 entries for add_ids_abstracts data\")\n",
    "print(\"----------------------------\")\n",
    "results.show()\n",
    "results.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
