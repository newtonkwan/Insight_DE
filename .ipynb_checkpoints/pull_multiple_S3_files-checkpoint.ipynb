{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This part attempts to pull multiple S3 files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)\n",
    "bucket_name = \"open-research-corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads all gz files from an AWS bucket\n",
    "def read_all_gz_from_bucket(bucket_name):\n",
    "    return spark.read.text(\"s3a://{0}/corpus-2019-01-31/*.gz\".format(bucket_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This part attempts to write a pyspark dataframe to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = \"s3a://open-research-corpus/sample-S2-records.gz\" # path to the example file from S3 file \n",
    "raw_data = spark.read.text(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_aws_s3(bucket_name, file_name, df):\n",
    "    df.write.save(\"s3a://{0}/{1}\".format(bucket_name, file_name), format=\"txt\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"preprocessed-open-research-corpus\"\n",
    "file_name = \"sample-S2-records\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_aws_s3(bucket_name, file_name, raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This part attempts to read a pyspark dataframe from S3 as a json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"s3a://preprocessed-open-research-corpus/sample-S2-records/part-00000-9ebb5618-7074-4eab-9e80-1b86f286a8b9-c000.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_json_from_bucket(filename):\n",
    "    return spark.read.json(\"{0}\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads all JSON files from an AWS bucket\n",
    "def read_all_json_from_bucket(bucket_name):\n",
    "    return sql_context.read.json(\"s3a://{0}/*.json*\".format(bucket_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_all_json_from_bucket(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for raw data + ids + abstracts\n",
      "-------------------------------------\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "First 5 entries for add_ids_abstracts data\n",
      "----------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"entities\":[\"Epi...|\n",
      "|{\"entities\":[\"Lip...|\n",
      "|{\"entities\":[\"Spa...|\n",
      "|{\"entities\":[\"ACT...|\n",
      "|{\"entities\":[],\"j...|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Schema for raw data + ids + abstracts\")\n",
    "print(\"-------------------------------------\")\n",
    "df.createOrReplaceTempView(\"raw_ids_and_abstracts\")\n",
    "df.printSchema()\n",
    "results = spark.sql(\"SELECT * FROM raw_ids_and_abstracts LIMIT 5\")\n",
    "print(\"First 5 entries for add_ids_abstracts data\")\n",
    "print(\"----------------------------\")\n",
    "results.show()\n",
    "results.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
