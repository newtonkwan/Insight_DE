{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark import SparkContext\n",
    "import redis \n",
    "\n",
    "def get_id(line):\n",
    "    parenthesis = \"\\\"\" # string literal for \"\n",
    "    paper_id_tag = \"\\\"id\\\"\" # find the first occurence of \"id\"\n",
    "    id_label_start = line.find(paper_id_tag) # this is the index that the id label starts\n",
    "    id_tag_start = id_label_start + 6 # this is the index that the id tag starts. Always be 6.\n",
    "    id_tag_end = line.find(parenthesis, id_tag_start)  #this is the index that the id tag ends\n",
    "    id_tag = line[id_tag_start:id_tag_end] # id tag string \n",
    "    return id_tag\n",
    "\n",
    "def get_abstract(line):\n",
    "    parenthesis = \"\\\"\" # string literal for \"\n",
    "    paper_abstract_tag = \"\\\"paperAbstract\\\"\"\n",
    "    abstract_label_start = line.find(paper_abstract_tag) # index that the abstrat label starts \n",
    "    abstract_tag_start = abstract_label_start + 17 # the start of the abstract tag \n",
    "    abstract_tag_end = line.find(parenthesis, abstract_tag_start) # the end of the abstract tag\n",
    "    abstract_tag = line[abstract_tag_start:abstract_tag_end] # abstract tag string \n",
    "    return abstract_tag\n",
    "\n",
    "def adding_ids(df):\n",
    "    '''\n",
    "    This function takes the raw data dataframe and adds an id column for the dat a\n",
    "    Ex: \n",
    "    id \n",
    "    23402939423\n",
    "    02398402384\n",
    "    23402938402\n",
    "    '''\n",
    "    add_ids = df.withColumn(\"id\", get_id_udf(raw_data.value))\n",
    "    return add_ids\n",
    "\n",
    "def adding_abstracts(df):\n",
    "    add_ids_abstracts = df.withColumn(\"abstracts\", get_abstract_udf(raw_data.value))\n",
    "    return add_ids_abstracts\n",
    "\n",
    "# create a user defined function for get_id and get_abstract, which is compatable with a spark dataframe \n",
    "get_id_udf = udf(lambda line: get_id(line), StringType())\n",
    "get_abstract_udf = udf(lambda line: get_abstract(line), StringType())\n",
    "\n",
    "# read in the raw data file \n",
    "filenames = \"s3a://open-research-corpus/sample-S2-records.gz\" # path to the example file from S3 file \n",
    "sc = spark.sparkContext\n",
    "raw_data = spark.read.text(filenames)\n",
    "raw_and_ids = adding_ids(raw_data)\n",
    "raw_ids_abstracts = adding_abstracts(raw_and_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
