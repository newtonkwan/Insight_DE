{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a testing grounds for spark job mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import SQLContext, Row\n",
    "from pyspark import SparkContext\n",
    "import redis \n",
    "\n",
    "filenames = \"s3a://open-research-corpus/sample-S2-records.gz\" # path to the example file from S3 file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer type output\n",
    "square_udf_int = udf(lambda z: square(z), IntegerType())\n",
    "get_id_udf_string = udf(lambda z: get_id(z), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+--------------------+--------------------+--------+----+-----------------+\n",
      "|                  id|papername|           citations|                year|abstract|tags|           author|\n",
      "+--------------------+---------+--------------------+--------------------+--------+----+-----------------+\n",
      "|{\"entities\":[\"Epi...|  ovarian|cancer\",\"Excision...|admission\",\"Malig...|neoplasm|  of|ovary\",\"Morbidity|\n",
      "+--------------------+---------+--------------------+--------------------+--------+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using spark 2.1 \n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load a text file and convert each line to a Row.\n",
    "lines = sc.textFile(filenames)\n",
    "#parts = lines.map(lambda l: l.split(\",\"))\n",
    "#parts = lines.map(lambda l: l)\n",
    "parts = lines.map(lambda l:l.split())\n",
    "# Each line is converted to a tuple.\n",
    "people = parts.map(lambda p: (p[0], p[1], p[2], p[3], p[4], p[5], p[6]))\n",
    "\n",
    "# The schema is encoded in a string.\n",
    "schemaString = \"id papername citations year abstract tags author\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "schemaPeople = spark.createDataFrame(people, schema)\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Creates a temporary view using the DataFrame\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "results = spark.sql(\"SELECT * FROM people LIMIT 1\")\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- papername: string (nullable = true)\n",
      " |-- citations: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- abstract: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "type(schemaPeople)\n",
    "schemaPeople.printSchema()\n",
    "schemaPeople.withColumn('id', (get_id_udf_string(schemaPeople.id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(line):\n",
    "    parenthesis = \"\\\"\" # string literal for \"\n",
    "    paper_id_tag = \"\\\"id\\\"\" # find the first occurence of \"id\"\n",
    "    id_label_start = line.find(paper_id_tag) # this is the index that the id label starts\n",
    "    id_tag_start = id_label_start + 6 # this is the index that the id tag starts. Always be 6.\n",
    "    id_tag_end = line.find(parenthesis, id_tag_start)  #this is the index that the id tag ends\n",
    "    id_tag = line[id_tag_start:id_tag_end] # id tag string \n",
    "    return id_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a user defined function for get_id, which is compatable with a spark dataframe \n",
    "get_id_udf = udf(lambda line: get_id(line), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the raw data file \n",
    "raw_data = spark.read.text(filenames)\n",
    "#print(type(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for raw data\n",
      "-------------------\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n",
      "First 5 entries for raw data\n",
      "----------------------------\n",
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"entities\":[\"Epi...|\n",
      "|{\"entities\":[\"Lip...|\n",
      "|{\"entities\":[\"Spa...|\n",
      "|{\"entities\":[\"ACT...|\n",
      "|{\"entities\":[],\"j...|\n",
      "+--------------------+\n",
      "\n",
      "Schema for raw data\n",
      "-------------------\n",
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      "\n",
      "First 5 entries for add_ids data\n",
      "----------------------------\n",
      "+--------------------+--------------------+\n",
      "|               value|                  id|\n",
      "+--------------------+--------------------+\n",
      "|{\"entities\":[\"Epi...|4cbba8127c8747a3b...|\n",
      "|{\"entities\":[\"Lip...|4c61478345166be0d...|\n",
      "|{\"entities\":[\"Spa...|34ca6d85db744543d...|\n",
      "|{\"entities\":[\"ACT...|3316b8b97c1e17ac9...|\n",
      "|{\"entities\":[],\"j...|58ff17c7d8ca00673...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#grabbed_id = udf(lambda body: get_id(body), StringType())\n",
    "#partially_cleaned_data = raw_data.withColumn(\"id\", grabbed_id(\"id\"))\n",
    "#partially_cleaned_data = raw_data.withColumn(\"id\", decode_line(\"paper_id\"))\n",
    "print(\"Schema for raw data\")\n",
    "print(\"-------------------\")\n",
    "raw_data.createOrReplaceTempView(\"raw\")\n",
    "raw_data.printSchema()\n",
    "results = spark.sql(\"SELECT value FROM raw LIMIT 5\")\n",
    "print(\"First 5 entries for raw data\")\n",
    "print(\"----------------------------\")\n",
    "results.show()\n",
    "add_ids = raw_data.withColumn(\"id\", get_id_udf(raw_data.value))\n",
    "print(\"Schema for raw data\")\n",
    "print(\"-------------------\")\n",
    "add_ids.createOrReplaceTempView(\"raw_and_ids\")\n",
    "add_ids.printSchema()\n",
    "results = spark.sql(\"SELECT * FROM raw_and_ids LIMIT 5\")\n",
    "print(\"First 5 entries for add_ids data\")\n",
    "print(\"----------------------------\")\n",
    "results.show()\n",
    "#results = spark.sql(\"SELECT id FROM partiallycleaned LIMIT 10\")\n",
    "#ids = spark.sql(\"SELECT id FROM partiallycleaned LIMIT 10\")\n",
    "#abstracts = spark.sql(\"SELECT paperAbstract FROM partiallycleaned LIMIT 10\")\n",
    "#raw_data.printSchema()\n",
    "#years.show()\n",
    "#ids.show()\n",
    "#abstracts.show()\n",
    "#results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|         id|\n",
      "+-----------+\n",
      "|[B@7c01c4f5|\n",
      "|[B@4f687f60|\n",
      "|[B@5f334bbc|\n",
      "|[B@40eeb25e|\n",
      "|[B@32baf0da|\n",
      "|[B@3ea51a92|\n",
      "|[B@7b8ec851|\n",
      "|[B@4ec7e93c|\n",
      "|[B@2b1eec1e|\n",
      "|[B@3df91265|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grabbed_id = udf(lambda body: get_id(body), StringType())\n",
    "#partially_cleaned_data = raw_data.withColumn(\"id\", grabbed_id(\"id\"))\n",
    "#print(type(partially_cleaned_data))\n",
    "partially_cleaned_data.createOrReplaceTempView(\"partiallycleaned\")\n",
    "results = spark.sql(\"SELECT id FROM partiallycleaned LIMIT 10\")\n",
    "#partially_cleaned_data.printSchema()\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_papers(line):\n",
    "    id_label_start = line.find(paper_id_tag) # this is the index that the id label starts\n",
    "    id_tag_start = id_label_start + 6 # this is the index that the id tag starts. Always be 6.\n",
    "    id_tag_end = line.find(parenthesis, id_tag_start)  #this is the index that the id tag ends\n",
    "    id_tag = line[id_tag_start:id_tag_end] # id tag string \n",
    "    \n",
    "    abstract_label_start = line.find(paper_abstract_tag) # index that the abstrat label starts \n",
    "    abstract_tag_start = abstract_label_start + 17 # the start of the abstract tag \n",
    "    abstract_tag_end = line.find(parenthesis, abstract_tag_start) # the end of the abstract tag\n",
    "    abstract_tag = line[abstract_tag_start:abstract_tag_end] # abstract tag string\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_to_redis(rdd):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = \"s3a://open-research-corpus/sample-S2-records.gz\" # path to the example file from S3 file \n",
    "df_txt = sc.textFile(filenames) # reads and stores the data file \n",
    "df = df.WithColumn(\"hello\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_udf_int = udf(lambda z: square(z), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This file pulls 1GB .gz file of data from an S3 bucket\n",
    "'''\n",
    "rdb = redis.Redis(host=\"10.0.0.6\", port=6379) # set up the redis database connection \n",
    "filenames = \"s3a://open-research-corpus/sample-S2-records.gz\" # path to the example file from S3 file \n",
    "# uncomment sc when you are running a spark-submit job on the terminal \n",
    "#sc = SparkContext(appName = \"Pull Open Research Corpus\") # setup the Spark Context \n",
    "data = sc.textFile(filenames) # reads and stores the data file \n",
    "\n",
    "# setting up important variables \n",
    "parenthesis = \"\\\"\" # string literal for \"\n",
    "bracket = r\"]\" # look for \"]\"\n",
    "paper_id_tag = \"\\\"id\\\"\" # find the first occurence of \"id\"\n",
    "paper_year_tag = \"\\\"year\\\"\" # find the first occurence of \"year\"\n",
    "paper_citation_tag = \"\\\"inCitations\\\"\" # find the occurence of \"inCitations\"\n",
    "paper_abstract_tag = \"\\\"paperAbstract\\\"\"\n",
    "\n",
    "num_in_db = 0\n",
    "for line in data.take(len_of_data):\n",
    "        #line = line.encode('utf8') # encodes the unicode to ascii \n",
    "        # look for the labels \"id\", \"year\", \"inCitations\", and \"paperAbstract\"\n",
    "        id_label_start = line.find(paper_id_tag) # this is the index that the id label starts\n",
    "        year_label_start = line.find(paper_year_tag) # index that the year label starts \n",
    "        citation_label_start = line.find(paper_citation_tag) # index that the citation label starts \n",
    "        abstract_label_start = line.find(paper_abstract_tag) # index that the abstrat label starts \n",
    "\n",
    "        # look for the tag of each label \n",
    "        id_tag_start = id_label_start + 6 # this is the index that the id tag starts. Always be 6.\n",
    "        year_tag_start = year_label_start + 7 # index that the year tag starts \n",
    "        citation_tag_start = citation_label_start + 15 # index that the citation tag starts\n",
    "        abstract_tag_start = abstract_label_start + 17 # the start of the abstract tag \n",
    "\n",
    "        # look for the last index of each tag \n",
    "        id_tag_end = line.find(parenthesis, id_tag_start)  #this is the index that the id tag ends\n",
    "        year_tag_end = line.find(parenthesis, year_tag_start) - 1 # this is the index that the year tag ends\n",
    "        citation_tag_end = line.find(bracket, citation_tag_start)  # this is the index that the citation tag ends \n",
    "        abstract_tag_end = line.find(parenthesis, abstract_tag_start) # the end of the abstract tag\n",
    "    \n",
    "        # extract the tag \n",
    "        id_tag = line[id_tag_start:id_tag_end] # id tag string \n",
    "        if year_label_start == -1: # it didn't find the string year\n",
    "                year_tag = None\n",
    "        else:\n",
    "                year_tag = line[year_tag_start:year_tag_end] # year tag string\n",
    "        if citation_tag_start == citation_tag_end: # if there are no citations:\n",
    "                num_citations = 0\n",
    "        else:\n",
    "                citation_list = line[citation_tag_start:citation_tag_end].split(\",\") # make it a list, count number of entries\n",
    "                num_citations = len(citation_list) # number of citations\n",
    "        abstract_tag = line[abstract_tag_start:abstract_tag_end] # abstract tag string\n",
    "        \n",
    "        temp_tup = (year_tag, num_citations, abstract_tag) # temporarily store the (year, number of citation, abstract)\n",
    "        #q_json = json.dumps({\"id\": q.id, \"title\": q.title, \"min_hash\": q.min_hash, \"lsh_hash\": q.lsh_hash, \"timestamp\": q.creation_date})\n",
    "        #print(type(id_tag))\n",
    "        #rdb.lpush(id_tag, pickle.dumps(temp_tup))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_to_redis(rdd): \n",
    "    '''\n",
    "    takes a rdd and stores relevant information to redis database\n",
    "    '''\n",
    "    papers = rdd.collect() # an iterable list \n",
    "    rdb = redis.Redis(host=\"10.0.0.6\", port=6379) # set up the redis database connection \n",
    "    # setting the keywords that we search for\n",
    "    parenthesis = \"\\\"\" # string literal for \"\n",
    "    bracket = r\"]\" # look for \"]\"\n",
    "    paper_id_tag = \"\\\"id\\\"\" # find the first occurence of \"id\"\n",
    "    paper_year_tag = \"\\\"year\\\"\" # find the first occurence of \"year\"\n",
    "    paper_citation_tag = \"\\\"inCitations\\\"\" # find the occurence of \"inCitations\"\n",
    "    paper_abstract_tag = \"\\\"paperAbstract\\\"\"\n",
    "    \n",
    "    for paper in papers: \n",
    "        # look for the labels \"id\", \"year\", \"inCitations\", and \"paperAbstract\"\n",
    "        id_label_start = line.find(paper_id_tag) # this is the index that the id label starts\n",
    "        year_label_start = line.find(paper_year_tag) # index that the year label starts \n",
    "        citation_label_start = line.find(paper_citation_tag) # index that the citation label starts \n",
    "        abstract_label_start = line.find(paper_abstract_tag) # index that the abstrat label starts \n",
    "\n",
    "        # look for the tag of each label \n",
    "        id_tag_start = id_label_start + 6 # this is the index that the id tag starts. Always be 6.\n",
    "        year_tag_start = year_label_start + 7 # index that the year tag starts \n",
    "        citation_tag_start = citation_label_start + 15 # index that the citation tag starts\n",
    "        abstract_tag_start = abstract_label_start + 17 # the start of the abstract tag \n",
    "\n",
    "        # look for the last index of each tag \n",
    "        id_tag_end = line.find(parenthesis, id_tag_start)  #this is the index that the id tag ends\n",
    "        year_tag_end = line.find(parenthesis, year_tag_start) - 1 # this is the index that the year tag ends\n",
    "        citation_tag_end = line.find(bracket, citation_tag_start)  # this is the index that the citation tag ends \n",
    "        abstract_tag_end = line.find(parenthesis, abstract_tag_start) # the end of the abstract tag\n",
    "\n",
    "        #extract the tag\n",
    "        #id_tag = line[id_tag_start:id_tag_end] # id tag string \n",
    "        #year_tag = line[year_tag_start:year_tag_end] # year tag string\n",
    "        #citation_list = line[citation_tag_start:citation_tag_end].split(\",\") # make it a list, count number of entries\n",
    "        #num_citations = len(citation_list) # number of citations \n",
    "        #abstract_tag = line[abstract_tag_start:abstract_tag_end] # abstract tag string\n",
    "\n",
    "        # computation \n",
    "        id_tag = line[id_tag_start:id_tag_end] # id tag string \n",
    "        if year_label_start == -1: # it didn't find the string year\n",
    "            year_tag = None\n",
    "        else:\n",
    "            year_tag = line[year_tag_start:year_tag_end] # year tag string\n",
    "        if citation_tag_start == citation_tag_end: # if there are no citations:\n",
    "            num_citations = 0\n",
    "        else:\n",
    "            citation_list = line[citation_tag_start:citation_tag_end].split(\",\") # make it a list, count number of entries\n",
    "            num_citations = len(citation_list) # number of citations\n",
    "        abstract_tag = line[abstract_tag_start:abstract_tag_end] # abstract tag string\n",
    "        # this serializes the data using json.dumps so that it can be put into redis \n",
    "        #info = {\"year\":year_tag, \"citations\":num_citations, \"abstract\":abstract_tag}\n",
    "        #json_representation = json.dumps(info)\n",
    "        #rdb.set(id_tag, json_representation) # push to redis database \n",
    "        rdb.set(id_tag, abstract_tag)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis \n",
    "import json\n",
    "from pyspark import SparkContext \n",
    "'''\n",
    "This file pulls 1GB .gz file of data from an S3 bucket\n",
    "'''\n",
    "rdb = redis.Redis(host=\"10.0.0.6\", port=6379) # set up the redis database connection \n",
    "filenames = \"s3a://open-research-corpus/sample-S2-records.gz\" # path to the example file from S3 file \n",
    "\n",
    "# uncomment sc when you are running a spark-submit job on the terminal \n",
    "#sc = SparkContext(appName = \"Pull Open Research Corpus\") # setup the Spark Context \n",
    "file = sc.textFile(filenames) # reads and stores the data file \n",
    "rdd = file.flatMap(lambda line: line.split(\"\\n\")).map(lambda )\n",
    "#rdd.map(store_to_redis(rdd))\n",
    "\n",
    "\n",
    "#store_to_redis(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdb = redis.Redis(host=\"10.0.0.6\", port=6379) # set up the redis database connection \n",
    "rdb.get(\"06f3d20b2c9191b4f03761a61312a1c71345a003\")\n",
    "#unpacked_dict = json.loads(rdb.get('185e6a412b05973b280bdad525907eb1d12a0a37').decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = sc.textFile(\"hdfs://<public_dns>:9000/user/test.txt\")\n",
    "counts = file.flatMap(lambda line: line.split(\" \"))\\\n",
    "           .map(lambda word: (word, 1))\\\n",
    "           .reduceByKey(lambda a, b: a + b)\n",
    "res = counts.collect()\n",
    "for val in res:\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
